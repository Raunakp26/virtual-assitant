// src/pages/Home.jsx
import React, { useContext, useEffect, useRef, useState } from "react";
import { userDataContext } from "../context/UserContext";
import { useNavigate } from "react-router-dom";
import axios from "axios";

function Home() {
Â  const { userData, serverUrl, setUserData, getGeminiResponse } = useContext(userDataContext);
Â  const navigate = useNavigate();

Â  const [listening, setListening] = useState(false);
Â  const isSpeakingRef = useRef(false);
Â  const recognitionRef = useRef(null);
Â  const isRecognizingRef = useRef(false);
Â  const isMountedRef = useRef(true);
Â  const synth = window.speechSynthesis;

Â  // Function to check and log available voices
Â  const checkVoices = () => {
Â  Â  const voices = window.speechSynthesis.getVoices();
Â  Â  console.log("=== AVAILABLE VOICES ===");
Â  Â  voices.forEach((voice, index) => {
Â  Â  Â  console.log(`${index}: ${voice.name} (${voice.lang}) - Default: ${voice.default}`);
Â  Â  });
Â  Â  console.log("========================");
Â  };

Â  // Initialize voices when component mounts
Â  useEffect(() => {
Â  Â  if (window.speechSynthesis.getVoices().length > 0) {
Â  Â  Â  checkVoices();
Â  Â  } else {
Â  Â  Â  window.speechSynthesis.onvoiceschanged = checkVoices;
Â  Â  }
Â  }, []);

Â  const handleLogOut = async () => {
Â  Â  try {
Â  Â  Â  await axios.get(`${serverUrl}/api/auth/logout`, { withCredentials: true });
Â  Â  Â  setUserData(null);
Â  Â  Â  navigate("/signin");
Â  Â  } catch (error) {
Â  Â  Â  setUserData(null);
Â  Â  Â  console.log(error);
Â  Â  }
Â  };

Â  const startRecognition = () => {
Â  Â  if (isRecognizingRef.current) {
Â  Â  Â  console.log("Recognition already running, skipping start");
Â  Â  Â  return;
Â  Â  }
Â  Â Â 
Â  Â  if (isSpeakingRef.current) {
Â  Â  Â  console.log("Currently speaking, skipping recognition start");
Â  Â  Â  return;
Â  Â  }

Â  Â  if (!recognitionRef.current) {
Â  Â  Â  console.log("Recognition not initialized, skipping start");
Â  Â  Â  return;
Â  Â  }
Â  Â Â 
Â  Â  try {
Â  Â  Â  recognitionRef.current.start();
Â  Â  Â  console.log("Recognition start requested");
Â  Â  } catch (error) {
Â  Â  Â  if (error.name !== "InvalidStateError") {
Â  Â  Â  Â  console.error("Recognition start error:", error);
Â  Â  Â  }
Â  Â  }
Â  };

Â  const stopRecognition = () => {
Â  Â  if (isRecognizingRef.current) {
Â  Â  Â  try {
Â  Â  Â  Â  recognitionRef.current?.stop();
Â  Â  Â  Â  console.log("Recognition stop requested");
Â  Â  Â  } catch (error) {
Â  Â  Â  Â  console.error("Recognition stop error:", error);
Â  Â  Â  }
Â  Â  }
Â  };

Â  const resetRecognition = () => {
Â  Â  console.log("Resetting recognition state");
Â  Â  stopRecognition();
Â  Â  isRecognizingRef.current = false;
Â  Â  setListening(false);
Â  Â  setTimeout(() => {
Â  Â  Â  if (isMountedRef.current && !isSpeakingRef.current) {
Â  Â  Â  Â  startRecognition();
Â  Â  Â  }
Â  Â  }, 1000);
Â  };

Â  const speak = (text) => {
Â  Â  console.log("Speaking:", text);
Â  Â  synth.cancel();
Â  Â  const utterance = new SpeechSynthesisUtterance(text);
Â  Â  utterance.lang = "en-US";
Â  Â  const voices = window.speechSynthesis.getVoices();
Â  Â  const englishVoice = voices.find((v) => v.lang.startsWith("en-") && v.default) ||Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  voices.find((v) => v.lang.startsWith("en-")) ||
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  voices[0];
Â  Â  if (englishVoice) {
Â  Â  Â  utterance.voice = englishVoice;
Â  Â  Â  console.log("Using voice:", englishVoice.name);
Â  Â  }

Â  Â  utterance.rate = 0.9;
Â  Â  utterance.pitch = 1.0;
Â  Â  utterance.volume = 1.0;

Â  Â  isSpeakingRef.current = true;
Â  Â Â 
Â  Â  utterance.onstart = () => {
Â  Â  Â  console.log("Speech started");
Â  Â  };
Â  Â Â 
Â  Â  utterance.onend = () => {
Â  Â  Â  console.log("Speech ended");
Â  Â  Â  isSpeakingRef.current = false;
Â  Â  Â  setTimeout(() => {
Â  Â  Â  Â  if (!isRecognizingRef.current && isMountedRef.current) {
Â  Â  Â  Â  Â  console.log("Restarting recognition after speech");
Â  Â  Â  Â  Â  startRecognition();
Â  Â  Â  Â  }
Â  Â  Â  }, 1000);
Â  Â  };
Â  Â Â 
Â  Â  utterance.onerror = (event) => {
Â  Â  Â  console.error("Speech error:", event.error);
Â  Â  Â  isSpeakingRef.current = false;
Â  Â  Â  setTimeout(() => {
Â  Â  Â  Â  if (!isRecognizingRef.current && isMountedRef.current) {
Â  Â  Â  Â  Â  console.log("Restarting recognition after speech error");
Â  Â  Â  Â  Â  startRecognition();
Â  Â  Â  Â  }
Â  Â  Â  }, 1000);
Â  Â  };

Â  Â  synth.speak(utterance);
Â  };

Â  const handleCommand = (data, transcript) => {
Â  Â  const { type, response } = data;
Â  Â  const userInput = data.userInput || transcript;

Â  Â  console.log("=== HANDLING COMMAND ===");
Â  Â  console.log("Type:", type);
Â  Â  console.log("Response:", response);
Â  Â  console.log("User Input:", userInput);
Â  Â  console.log("========================");

Â  Â  if (!response || response.trim() === "") {
Â  Â  Â  console.error("No response to speak!");
Â  Â  Â  return;
Â  Â  }

Â  Â  speak(response);

Â  Â  if (type === "google_search") {
Â  Â  Â  const query = encodeURIComponent(userInput);
Â  Â  Â  window.open(`https://www.google.com/search?q=${query}`, "_blank");
Â  Â  }

Â  Â  if (type === "calculator_open") {
Â  Â  Â  window.open("https://www.google.com/search?q=calculator", "_blank");
Â  Â  }

Â  Â  if (type === "instagram_open") {
Â  Â  Â  window.open("https://www.instagram.com/", "_blank");
Â  Â  }

Â  Â  if (type === "facebook_open") {
Â  Â  Â  window.open("https://www.facebook.com/", "_blank");
Â  Â  }

Â  Â  if (type === "weather-show") {
Â  Â  Â  window.open("https://www.google.com/search?q=weather", "_blank");
Â  Â  }

Â  Â  if (type === "youtube_search" || type === "youtube_play") {
Â  Â  Â  const query = encodeURIComponent(userInput);
Â  Â  Â  window.open(`https://www.youtube.com/results?search_query=${query}`, "_blank");
Â  Â  }
Â  };

Â  const safeRecognition = () => {
Â  Â  if (!isSpeakingRef.current && !isRecognizingRef.current) {
Â  Â  Â  startRecognition();
Â  Â  } else {
Â  Â  Â  console.log("Skipping recognition start - speaking or already recognizing");
Â  Â  }
Â  };

  const fallbackIntervalRef = useRef(null);

  // New function to start the entire process
  const startJarvis = () => {
    // Check if recognition is already running to prevent duplicates
    if (isRecognizingRef.current) {
      console.log("Jarvis is already running.");
      return;
    }

    // Start recognition and speech
    speak("Hello there! How can I help you today?");
    safeRecognition();

    // Start the fallback interval
    fallbackIntervalRef.current = setInterval(() => {
      if (!isSpeakingRef.current && !isRecognizingRef.current && isMountedRef.current) {
        console.log("Fallback: restarting recognition");
        safeRecognition();
      }
    }, 15000);
  };

  // Main Effect for initializing SpeechRecognition API
  useEffect(() => {
    if (recognitionRef.current) {
      console.log("Recognition already initialized, skipping");
      return;
    }

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const recognition = new SpeechRecognition();

    recognition.continuous = true;
    recognition.lang = "en-US";
    recognitionRef.current = recognition;

    recognition.onstart = () => {
      console.log("Recognition started");
      isRecognizingRef.current = true;
      setListening(true);
    };

    recognition.onend = () => {
      console.log("Recognition ended");
      isRecognizingRef.current = false;
      setListening(false);
      if (!isSpeakingRef.current && isMountedRef.current) {
        console.log("Scheduling recognition restart");
        setTimeout(safeRecognition, 2000);
      }
    };

    recognition.onerror = (event) => {
      console.warn("Recognition error:", event.error);
      isRecognizingRef.current = false;
      setListening(false);
      if (event.error !== "aborted" && !isSpeakingRef.current && isMountedRef.current) {
        console.log("Scheduling recognition restart after error");
        setTimeout(safeRecognition, 3000);
      }
    };

    recognition.onresult = async (e) => {
      const transcript = e.results[e.results.length - 1][0].transcript.trim();
      console.log("Heard:", transcript);
      if (userData?.assistantName && transcript.toLowerCase().includes(userData.assistantName.toLowerCase())) {
        console.log("Assistant name detected, processing command...");
        stopRecognition();
        isRecognizingRef.current = false;
        setListening(false);
        try {
          const data = await getGeminiResponse(transcript);
          console.log("Assistant response:", data);
          if (data?.response) {
            console.log("Assistant says:", data.response);
            handleCommand({ ...data, userInput: data.userInput || transcript }, transcript);
          } else {
            console.log("No response from assistant");
            speak("I'm sorry, I couldn't understand that. Please try again.");
          }
        } catch (error) {
          console.error("Error getting assistant response:", error);
          speak("I'm sorry, there was an error processing your request. Please try again.");
        }
      }
    };

    return () => {
      console.log("Cleaning up recognition");
      isMountedRef.current = false;
      stopRecognition();
      setListening(false);
      isRecognizingRef.current = false;
      // Clear the fallback interval on unmount
      if (fallbackIntervalRef.current) {
        clearInterval(fallbackIntervalRef.current);
      }
    };
  }, []);

  useEffect(() => {
    if (userData?.assistantName) {
      console.log("User data updated, assistant name:", userData.assistantName);
    }
  }, [userData]);

  return (
    <div className="w-full h-screen bg-gradient-to-t from-black to-[#030353] flex flex-col justify-center items-center relative">
      <div className="absolute top-6 right-6 flex gap-3">
        <button
          className="px-5 py-2 bg-white text-black font-semibold rounded-full shadow-md hover:bg-gray-200 transition"
          onClick={handleLogOut}
        >
          Log Out
        </button>
        <button
          className="px-5 py-2 bg-white text-black font-semibold rounded-full shadow-md hover:bg-gray-200 transition"
          onClick={() => navigate("/customize")}
        >
          Customize
        </button>
      </div>

      {/* NEW: Start Button */}
      {!(listening || isSpeakingRef.current) && (
        <button
          className="px-8 py-4 mb-8 bg-blue-600 text-white font-bold text-xl rounded-full shadow-xl hover:bg-blue-700 transition transform hover:scale-105"
          onClick={startJarvis}
        >
          Start Jarvis
        </button>
      )}

      <div className="w-[300px] h-[400px] flex flex-col items-center overflow-hidden rounded-3xl shadow-lg bg-white/10">
        <img src={userData?.assistantImage} alt="Assistant" className="h-full w-full object-cover" />
      </div>

      <h1 className="text-white text-2xl mt-6">
        I'm <span className="font-bold">{userData?.assistantName}</span>
      </h1>

      <p className="mt-4 text-sm text-gray-300">
        {listening ? "ðŸŽ¤ Listening..." : "ðŸ›‘ Not Listening"}
      </p>

      {/* You can re-enable these if you need them for debugging */}
      {/*
      <div className="mt-4 flex gap-2">
        <button
          className="px-4 py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600 transition"
          onClick={() => speak("Hello! This is a test of the voice system.")}
        >
          Test Voice
        </button>
        <button
          className="px-4 py-2 bg-green-500 text-white rounded-lg hover:bg-green-600 transition"
          onClick={resetRecognition}
        >
          Restart Recognition
        </button>
        <button
          className="px-4 py-2 bg-red-500 text-white rounded-lg hover:bg-red-600 transition"
          onClick={() => {
            stopRecognition();
            console.log("Recognition manually stopped");
          }}
        >
          Stop Recognition
        </button>
      </div>
      */}
    </div>
  );
}

export default Home;
